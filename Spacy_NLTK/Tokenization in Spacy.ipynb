{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7e0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db20ab66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\n",
      "Mariam\n",
      "loves\n",
      "sarma\n",
      "of\n",
      "Turkey\n",
      ".\n",
      "Khadidja\n",
      "loves\n",
      "eating\n",
      "Chadian\n",
      "kichen\n"
     ]
    }
   ],
   "source": [
    "nlp= spacy.blank(\"en\")\n",
    "\n",
    "doc= nlp(\"Mr. Mariam loves sarma of Turkey. Khadidja loves eating Chadian kichen\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fa3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(\"Tony gave two $ to peter.\")\n",
    "token0=doc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb13e57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony\n"
     ]
    }
   ],
   "source": [
    "print(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47a996a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1924f025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400cbc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f33b26d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2= doc[2]\n",
    "token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517695a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f8e19d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "($, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3= doc[3]\n",
    "token3,token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44953dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaec7a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index:  0 is_alpha:  True is_punct: False like_num : False is_currency:  False\n",
      "gave ==> index:  1 is_alpha:  True is_punct: False like_num : False is_currency:  False\n",
      "two ==> index:  2 is_alpha:  True is_punct: False like_num : True is_currency:  False\n",
      "$ ==> index:  3 is_alpha:  False is_punct: False like_num : False is_currency:  True\n",
      "to ==> index:  4 is_alpha:  True is_punct: False like_num : False is_currency:  False\n",
      "peter ==> index:  5 is_alpha:  True is_punct: False like_num : False is_currency:  False\n",
      ". ==> index:  6 is_alpha:  False is_punct: True like_num : False is_currency:  False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i,\n",
    "         \"is_alpha: \", token.is_alpha,\n",
    "         \"is_punct:\", token.is_punct,\n",
    "         \"like_num :\", token.like_num,\n",
    "         \"is_currency: \", token.is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9de74861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virat@kohli.com', 'maria@sharapova.com', 'serena@williams.com', 'joe@root.com']\n"
     ]
    }
   ],
   "source": [
    "text= 'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'\n",
    "\n",
    "doc= nlp(text)\n",
    "emails= []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e37ab2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meryem True False\n",
      "yemek True False\n",
      "yemeye True False\n",
      "çok True False\n",
      "seviyor True False\n",
      ". False False\n",
      "yaprak True False\n",
      "sarması True False\n",
      "onun True False\n",
      "en True False\n",
      "sevdiği True False\n",
      "türk True False\n",
      "yemeğidir True False\n"
     ]
    }
   ],
   "source": [
    "nlp= spacy.blank('tr')\n",
    "doc= nlp(\"Meryem yemek yemeye çok seviyor. yaprak sarması onun en sevdiği türk yemeğidir\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.is_alpha, token.like_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e803fbb",
   "metadata": {},
   "source": [
    "### Customization spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28434421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= nlp('gimme double cheese extra large healthy pizza')\n",
    "\n",
    "tokens= [token.text for token in doc]\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2254f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens= [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec9f83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\n",
      "Mariam\n",
      "loves\n",
      "sarma\n",
      "of\n",
      "Turkey\n",
      ".\n",
      "Khadidja\n",
      "loves\n",
      "eating\n",
      "Chadian\n",
      "kichen\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Mr. Mariam loves sarma of Turkey. Khadidja loves eating Chadian kichen\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f2ccc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2292a732c00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8759c017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'sentencizer']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b504eda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Mariam loves sarma of Turkey.\n",
      "Khadidja loves eating Chadian kichen\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Mr. Mariam loves sarma of Turkey. Khadidja loves eating Chadian kichen\")\n",
    "\n",
    "for token in doc.sents:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ff19c5e",
   "metadata": {},
   "source": [
    "Exercise\n",
    "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
    "\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93d83d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "133a7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f153666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Look for data to help you address the question.\n",
      "Governments are good\n",
      "sources because data from public research is often freely available.\n",
      "Good\n",
      "places to start include http://www.data.gov/, and http://www.science.\n",
      "\n",
      "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
      "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
      "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(text)\n",
    "\n",
    "for token in doc.sents:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d09365cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.tokenizer_exceptions import URL_PATTERN\n",
    "dataset_urls= []\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        dataset_urls.append(token.text.strip())\n",
    "dataset_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39dd7b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "data_websites = [token.text for token in doc if token.like_url ] \n",
    "data_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad8924fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TOKEN', 'Look')\n",
      "('TOKEN', 'for')\n",
      "('TOKEN', 'data')\n",
      "('TOKEN', 'to')\n",
      "('TOKEN', 'help')\n",
      "('TOKEN', 'you')\n",
      "('TOKEN', 'address')\n",
      "('TOKEN', 'the')\n",
      "('TOKEN', 'question')\n",
      "('SUFFIX', '.')\n",
      "('TOKEN', 'Governments')\n",
      "('TOKEN', 'are')\n",
      "('TOKEN', 'good')\n",
      "('TOKEN', 'sources')\n",
      "('TOKEN', 'because')\n",
      "('TOKEN', 'data')\n",
      "('TOKEN', 'from')\n",
      "('TOKEN', 'public')\n",
      "('TOKEN', 'research')\n",
      "('TOKEN', 'is')\n",
      "('TOKEN', 'often')\n",
      "('TOKEN', 'freely')\n",
      "('TOKEN', 'available')\n",
      "('SUFFIX', '.')\n",
      "('TOKEN', 'Good')\n",
      "('TOKEN', 'places')\n",
      "('TOKEN', 'to')\n",
      "('TOKEN', 'start')\n",
      "('TOKEN', 'include')\n",
      "('URL_MATCH', 'http://www.data.gov/')\n",
      "('SUFFIX', ',')\n",
      "('TOKEN', 'and')\n",
      "('URL_MATCH', 'http://www.science')\n",
      "('SUFFIX', '.')\n",
      "('TOKEN', 'gov/')\n",
      "('SUFFIX', ',')\n",
      "('TOKEN', 'and')\n",
      "('TOKEN', 'in')\n",
      "('TOKEN', 'the')\n",
      "('TOKEN', 'United')\n",
      "('TOKEN', 'Kingdom')\n",
      "('SUFFIX', ',')\n",
      "('URL_MATCH', 'http://data.gov.uk/.')\n",
      "('TOKEN', 'Two')\n",
      "('TOKEN', 'of')\n",
      "('TOKEN', 'my')\n",
      "('TOKEN', 'favorite')\n",
      "('TOKEN', 'data')\n",
      "('TOKEN', 'sets')\n",
      "('TOKEN', 'are')\n",
      "('TOKEN', 'the')\n",
      "('TOKEN', 'General')\n",
      "('TOKEN', 'Social')\n",
      "('TOKEN', 'Survey')\n",
      "('TOKEN', 'at')\n",
      "('URL_MATCH', 'http://www3.norc.org/gss+website/')\n",
      "('SUFFIX', ',')\n",
      "('TOKEN', 'and')\n",
      "('TOKEN', 'the')\n",
      "('TOKEN', 'European')\n",
      "('TOKEN', 'Social')\n",
      "('TOKEN', 'Survey')\n",
      "('TOKEN', 'at')\n",
      "('URL_MATCH', 'http://www.europeansocialsurvey.org/.')\n"
     ]
    }
   ],
   "source": [
    "custom_infixes = [URL_PATTERN[1:-1]] + list(nlp.Defaults.infixes)\n",
    "nlp.tokenizer.infix_finditer = spacy.util.compile_infix_regex(custom_infixes).finditer\n",
    "for t in nlp.tokenizer.explain(nlp(text).text):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156d44e",
   "metadata": {},
   "source": [
    "##### Figure out all transactions from this text with amount and currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e760b733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "doc= nlp(transactions)\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text, doc[token.i+1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724851f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0642ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
